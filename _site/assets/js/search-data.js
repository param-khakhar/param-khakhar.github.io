var store = [{
        "title": "Data Science, What?",
        "excerpt":" Data Science is an inter-disciplinary field which is used to gain insights into data through computation, statistics and visualization. Inter-disciplinary as in, The stuff present in the background are the popular softwares and frameworks use to carry out different tasks. Tasks such as? It turns out that, the entire process of gaining insights from the data comprises of several smaller sub tasks such as (Detailed Info in the respective articles): Ask an interesting question Here the question refers the task which you want to achieve in the end. It could be either be prediction or estimation of a variable depending on the past trends, it could also be a particular business question which you want to find an answer of like consumer resoponse for a particular product, grouping of consumers for different products etc. Get the Data In order to derive insigts, you would certainly need the data in the first place! Well, you yourself might be collecting the data so that you would use it now. It may also be the case that you want to use the data collected by others and in that scenario you may scrape the data either from web sites, APIs, online repositories etc. While getting the data you also need have to take care about how the data is sampled, or if there are any privacy issues which you might unintentionally violate. Also you need to take care about the relevance of the data you are using. After getting the data you may also need to clean the data. Cleaning as in if there are partially filled entries you may want to remove them. If the datatypes in the data are inconsistent you’d correct it. Exploratory Data Analysis The most important task among all the tasks! Well EDA is getting all about getting a flavor of the data which you’d be using. EDA involves usage of visualization techniques in order to find out any anomalies or patterns in the data. You can do so by checking the different statistical measures such as mean, std, median etc. Certaing modeling techniques (to be discussed afterwards) also involve removal of highly correlated features (in other words those features which significantly depend on each other). The removal of correlated features and also the selection of relevant features to be used for modeling purposes. Model the data This is the task about which you may hear in various articles as “Machine Learning”, “Neural Networks”, “Regression”, “Classifiers” etc. Now, Machine Learning is a sub-field of CS which comprises of algorithms which improve their performance with data. Here the performance may refer to predicting a dependent variable given the values of several independent variables, grouping the data based by finding certain common features, take a decision etc. Such algorithms are known as learning algorithms, this is because in a way the algorithm learns the relation between the dependent and independent variables. These learning algorithms when trained with data. The learning algorithms require data to update their hypothesis relation in order to make better predictions. This is known as the training of the model. Communicate and Visualize the results After you are done with the model forming part, you would certainly want to use it for the prediction of some other stuff. We need to make certain sanity checks whether the predictions made are sound enough or not. In this task you’d prefer to use certain visualization tools. So we use visualizations firstly for EDA and secondy for communicating the results. Well turns out that there’s so much buzz in the media about this, and one of the reasons for this is,   Data Scientist is the sexiest jobs of the 21st Century.   Harvard Business ReviewYou can check this Harvard Business Review article. Is it really the case?   I am a data scientist…I don’t find my job sexy.I am 40% a vacuum, another 40% a janitor.And the last 20%… A fortune-teller.   Jingles (Hong jing)You can check this article as well by Jingles (Hong jing). Well, there are opinions but, I really find it cool! Finally the roadmap!  References:   CS109 Data Science, Harvard University","categories": ["Data Science"],
        "tags": ["Intro","Data Science","What"],
        "url": "/data%20science/intro-data-science/"
      },{
        "title": "Why is statistics such a fascinating subject?",
        "excerpt":" The world is certainly uncertain, and statistics is a subject to estimate this uncertainity! Yes, no other field of study can deal with such uncertainity, there are always certain assumptions. In this and some of the following articles I’d write about some of the aspects of statistics the knowledge of whose can very much help to better understand the numbers, claims, etc. which are made by people. Statistical thinking can certainly help you in making wiser decisions, understand and judge the trustworthyness of different personel, and help you become better citizens, consumers, voters, etc. Even in academics, most subjects involve the study of chance events and as a result you need to know the knowledge of statistics. So even it’d help you become better academicians!   Being a statistician [means] you get to play in everyone’s backyard.   John TurkeyIn order to better understand the process of statistical analysis, suppose that you yourself are a statistician at a company and you have collected data for the number of employees that are absent on a given day in a company. You have collected the daily records for over 30 weeks. Now probably you would look for some patterns and even before that wonder what patterns might even exist. A simple organization such as grouping the number of absentees by days of the week may give you some insights. Doing that you notice that Mondays and Fridays are the days which have higher number of employees absent then the other days on average and also you can group the days of the week in two with Monday and Friday in one and the rest all weekdays in the other. Creating different summaries of the data is known as statistical description. You may also use certain visualization techniques in order to discover some pattern. Other than methods used for statistical description, there are also methods which the statisticians can use in order to generalize the detected patterns in a wider setting. This is known as statistical inference. Now what makes this valueable is that this comes with an objective measure of the likelihood that it is correct. However, we can never by sure that a generalization is correct, because uncertainity is so pervasive in the real world. Now returning to your original task, your hypothesis of higher absence rates on Monday and Fridays can statistically tested and we can also obtain the uncertainity of it being correct. The alternate hypothesis would then be that the data obtained is just by chance and no such concrete generalization exists. Interestingly, the wider setting in the generalization can also refer to future and in a way we can predict future! (with some uncertainity). If your hypothesis turns out to be correct you can certainly notify the board and go home. Now, all the above stuff may seem to be paradoxical as what are the rules which you use to make predictions about the future given the uncertainities of the world. The thing is that statistics involves logic rules which are different from the traditional mathematics. Essentially, statistical description and statistical inference are the workday roles of statistics. There are several other byways, some of which are also aren’t discussed in the curriculum such as:   Paradoxes of probability and statistics  Problems of using standard statistical techniques in non-standard situations.  Social Impact of StatisticsHow Statistics is different from Mathematics?       Turns out that statisticians solve the problem in a real life setting in contrast to mathematicians. Mathematical problems, abstracts out from all the uncertainity and fthen proceeds to report a unique solution. However, when we incorporate chances to play their role, the solution derived from maths is simply an approximation from the true value. Statisticians also calculate an answer and that too is an approximation but with the answer, a statistician also reports an uncertainity. Thus, the answer would be good only if the uncertainity is less.         Mathematicians also need data and they plug it in some general theorem and hence obtain the result. However, statisticians proceed the other way round. They are given a particular sample of information (from the entire population) and their job is to estimate the population from the obtained sample. Thus, for mathematicians data refers to the values of non-random variables whereas for statisticians, data refers to the values of random variables. Consequently, statisticians would have to take care about erroneous data which might be present in the example which they have.   Trivia Consider 3 cities A,B and C. The annual average temperatures (in degree Celsius) are 11.7, 25.2 and 27 respectively. Can we infer from this information that B is twice as hot as A in a year? Also would it be correct to say that B and C have similar temperatures throughout the year? Well, No. The thing is we can’t and we shouldn’t limit ourselves to the comparisons of the annual means. It may turn out that during some months B is slightly hotter than A but that trend is not preserved throughout the year. Also it might turn out that B and C have significantly different temperatures throughout a particular month. It often turns out that the byways are often more interesting then the stuff discussed in the curriculum, so stay tuned. Reference   A Panorama of Statistics","categories": ["Statistics"],
        "tags": ["Intro","Statistics"],
        "url": "/statistics/intro-stats/"
      },{
        "title": "Statistics is indeed paradoxical!",
        "excerpt":" Consider the following situation, You are looking for a hospital for the treatment of someone elderly in your family. There are two prominent choices available, Hospital A and Hospital B. For the last 1000 patients those which got treated from Hospital A had a survival rate of 90% whereas those which got treated from Hospital B had survival rate of 80%. So, Hospital A seems to be a clear winner right? Well it may not be the case. One shouldn’t ignore the fact that all the patients which arrive at the hospital do not have same health level. For example we can classify the patients having good health and bad health repectively. Let the number of survival rates of the patients considering the above classification scheme be as follows:   Hospital A - 900 in good health out of which 830 survived, 100 in poor health out of which 30 survived  Hospital B - 600 in good health out of which 590 survived, 400 in poor health out of which 210 survivedInterestingly, the survival rates of the patients having poor health is 52.5% in B and 30% in A. Amazingly, the survival rates of the patients having good health for Hospital A is 92.22 % whereas the same for Hospital B is 98.33%. Turns out that Hospital B is a clear winner and that too convincingly. The above is an example of Simpson’s paradox which occurs when the aggregated data hides a conditional variable, which is hidden additional factor that significantly influences results.   Statistics are persuasive. So much so that people, organizations, and whole countries base some of their most important decisions on organized data. But any set of statistics might have something lurking inside it that can turn the results completely upside down.   Mark Liddel &mdash; Ted-Ed, How statistics can be misleadingSimpson’s paradox isn’t hypothetical. You can look at Mark Liddel’s video for more examples. The above is one of the many things about statistics which I found very fascinating. ","categories": ["Statistics"],
        "tags": ["Paradox","Statistics","Data Science"],
        "url": "/statistics/paradox-stats/"
      },{
        "title": "My Learning Track",
        "excerpt":" Apart from my univesity curriculum, I do spend time learning stuff online in my free time. However, there are countless resources which you can get online, be it online courses, videos, podcasts, blogs etc which’d help you to learn stuff. In this article, I’d mention the resources which I used along with their concise description. This article is indeed a live one, and would get timely updates after completing new milestones :). The list mainly consists of the data science related stuff. Online Courses:-       Elements of AI:This was my first introduction to this new world where I came to know about Artificial Intelligence, Machine Learning, Deep Learning, Alan Turing, etc. It is a completely theoretical course and the main purpose is to get you acquainted with these buzz words. There are mentions about how this field evolved with time and contributions of some prominent researchers as well. There is a high level desciptions about Neural Networks and also about their cool applications. Do go for it if you don’t have in-depth idea about what these buzzwords mean and are curios to know them.         Machine Learning:Arguably one of the most famous course on Machine Learning out there on the internet. This can be attributed to the popularity of the instructor Andrew Ng, a world class AI researcher. This course is a subtle introduction to Machine Learning, and introduces about the algorithms which facilitate predictions. This course has assignments as well as quizzes which in my opinion are on the easier side, but nevertheless they are worth your time. This course is for those who aren’t familiar with Machine Learning algorithms as such. If you aren’t in this category than you may choose not to do it and proceed for the next one. Another reason for the popularity of this course is the pre-requisites is Introductory Linear Algebra and some knowledge about algorithms. However, this course uses Octave/Matlab as languages for Programming Assignments contrary to Python and R which are the languages which are mostly used.         CS109-Data Science:One of the not-so-popular online course but, believe me it is one of the best. The reasons why I am saying this is that it covers everything, you should have knowledge about if you want to become a data scientist someday. You’d learn Data Scraping, Data Processing, Data Visualization, Machine Learning, and Storytelling! The homework assignments are awesome projects on real datasets. However, the prereqs for this is Probability and Statistics which’d be covered in any intro-level course in your college. All the assignments use Python 2, and you’d have to figure out the Python3 equivalent, but it shouldn’t be too much of an issue. I highly recommend going for this if you satisfy the pre-reqs.         Introduction to Deep Learning:It’s an okayish course and it briefly introduces the various neural net architectures. The framework used for assignments is Tensorflow, and Keras. I found their explanation of backpropogation very cool. Their explanation of TensorFlow as a framework is also very lucid and intuitive. In rest of the course, they discuss CNNs, LSTMs, RNNs, Autoencoders, etc. Some of the assignments can be difficult for those who are using TensorFlow for the first time or using Neural Nets for the first time.         How to Win a Data Science Competition:This is an amazing course, targeted for those who know how to use Machine Learning in Python, but they want to hone their skills. What I mean is simply using model.fit() followed by model.predict() isn’t sufficient. There are several other aspects such as Feature Engineering, Hyperparameter tuning, Model Selection, Validation Strategies, etc which one needs to take care of while developing models which generalize well. This course covers all these other aspects. To be honest, this course is mainly for people who want to compete in data science competitions and get good scores but it’d certainly help you become a better data scientist at your workplace.   Blogs to Follow:   Towards Data Science: If you’re a newbie in this field and don’t understand any particular concept related to probability, statistic, machine learning, etc. this is the place where you’d most probably find a blog post related to that. The writers also write about new cool applications and what it feels to work in the industry.  Machine Learning Mastery - By Jason Brownlie Ph.D: Here you’d find many relevant blog posts for Machine Learning.  Kaggle Discussion: Kaggle organizes data science competitions but apart from that it has an excellent forum for discussing the advances in this area. You’d be able to learn new tricks and hacks from the experts and also interact with them.","categories": ["Data Science"],
        "tags": ["Statistics","Data Science"],
        "url": "/data%20science/learn-data-science/"
      },{
        "title": "Intern Season!",
        "excerpt":" Well, an article after a really really long time, reason being, I was taking exams, making submissions, giving demos, all with a compressed schedule, and after which there were the interviews for the summer internship 2021. Finally, things have settled (momentarily :P) and now the new semester would resume from 28th of September, but boy, I must say, this period was unprecedented (at least for me), and unanticipated as well. This article is mainly for the interview procedure for the internships and how should one prepare (or perhaps some of the mistakes which I did, which you shouldn’t do) in order to clear all the rounds. Note: I’m a computer science student (junior undergrad during the time at which this is written), and my applications was mainly for the companies who offered quant based roles, and software engineering (more formally systems engineering). Round 1 For most of the companies I applied, the first round would be a programming round wherein you’d have to write programs for the questions provided within a given time limit. Here, time could be collective for all the questions or might be the case that the individual questions have particular time frame in which they need to be solved. They are conducted on platforms such as hackerrank, hackerearth, codeground, codesignal etc. The shortlisting criteria for the companies seems to be a mix of number of questions solved and your CV/GPA i.e if you have a high GPA and solve less questions might get you shortlisted. “Seems” because this is a hypothesis, and yes I have observed some exceptions as well. The recommended language is C++ which is because of its speed and the data structures in the STL. You should know IO operations, rounding operations (for floats), and other string related functions such as substring etc, and declaring and using various data structures. Some questions may involve processing with large numbers, and in that scenarios, I used to switch to Python3, but knowing how to do that with C++ is also a plus. Talking about myself, this round got the better of me in majority of the companies, reason being, lack of sufficient practice. On asking some of the friends of mine who cracked this round more comfortably than me, one of the most common answer was that, “I had solved this particular question or a similar question earlier”. Now, in a timed exam, if you are spending time figuring out the various methods by which a particular question can be solved from scratch i.e amongst all possible different methods, then this won’t be a good idea. There are questions which can only be done using certain techniques and if you don’t know them, then there are very less chances of you writing the correct program. Well, you may pass some of the test cases, but not all! The solution for this would be to practice questions online from platforms such as Geeks For Geeks For Geeks, InterviewBit, and LeetCode. The main categories of the questions which I faced were:   Dynamic Programming  Graphs  Arrays  Strings  TreesA particular way of doing these questions would be to sort them by the number of times they are asked in the interviews, for obvious reasons ;). Along with them, it’d be certainly better if you participate in live contests, where in you’d have to solve as many questions (out of a givne number of questions) as you can in a given time limit. Codeforces, and Codechef are two of the prominent ones, used by people out there. You need to be sincere and diligent while solving the practice questions. The above recommendations are after assuming that you’ve taken a formal data structures course. If you haven’t done that, do it ASAP, it is quintessential! If you participate in contests out there, and do good, this round would be bread and butter for you, if you simply practice the standard problems. So go start coding! Simply mugging up solutions won’t be a good idea, because in the interviews you’d be asked on how do you form the solution, so try to understand and make things clear. The feeling after understanding the solution or a particular algorithm is deeply satisfying for me, and amazing as well :). Round 2 - 3 If you clear round 1, which for me was not easy, then you’d be quizzed by some interviewers. The questions would normally revolve around the above mentioned topics along with data structures such as hash maps (the different implementations), trees, and some algorithmic questions involving two pointers etc. So, time to revise your Data Structures course notes! If you are applying for a quant based position, then you’d be asked some questions based on probability as well. A must-do compulsory source for these puzzles is Brain Stellar, and we again have puzzles @ Geeks For Geeks. Solving them is fun :P. Along with these puzzles, you might be quizzed on typical probability questions involving random variables and expectation values, so you’d have to cover the probability basics from your probability course. Apart from this, interviewers tend to ask questions on the object oriented paradigm of programming as well, after all you being applying for an SDE role, should know it and their advantages over other paradigms. The title for this is round 2-3 because, you might be interviewed by 2 different interviewers over 2 different rounds. For one of my applications, it was probability and puzzles for the round 2, and some algorithmic questions in round 3. Round 4 This round is interviewed by the HR, wherein they check how you value their organization, what your moral values are, perhaps your ethics etc. I didn’t experience this round for my interviews so the only thing which I can tell is that be yourself and be confident. Other Remarks   I had stated this earlier, but again repeating this, start getting involved in competitive programming, if you haven’t started to do so. There are benefits in round 1 and round 2-3 as well.  It is indeed good to do some projects as well, but one thing which I noticed is that they would matter when you’ve cleared round 1, and performed satisfactorily in round 2-3. If these conditions aren’t satisfied, then projects won’t matter much imo. If there are two people under consideration after clearing the rounds, then the company would go with the candidate who has done relevant projects or rather the one with a higher GPA.  Another particular thing which you should practice is to precisely explaining the solution, verbally. This would matter as you’d verbally engage with the interviewers. You may get rejected for your inability to explain the anwer to your interviewer irrespective of the fact that you know the solution.  You should mention the time and space complexities of any algorithm which you explaing to your interviewer. Instead of directly jumping to think about the efficient/desired solution, you may choose to state the Brute Force solution (it’d help in case you aren’t able to formulate the efficient solution :P).  It might be a case that instead of the final answer, the interviewer is interested in your thinking procedure, in other words the approach which you are using to solve the problem, and the steps which you are taking to reach to the solution. It is a good habit to let your interviewers no about the method which you are thinking and why.  If you’re stuck, then you can ask for some hints i.e directions in which you should think for the solution.  Stay confident during the interview, be attentive, and have a good sleep. It’s okay to have a bad interview, but stressing out over it isn’t good. Well, after all it isn’t the end of the world or end of your opportunities! Learn from your experience and move on, I’d suggest.  You should know your CV inside out! Yes, you would be quizzed on it, and your inability to answer the questions on your projects could result in a bad impression. For projects you maybe asked on their motivation, challenges faced by you, and future improvisations.Happy Interviewing! All the points mentioned above, have been assembled from an ensemble of experiences of my seniors, my colleagues, and me as well. So a big shout out to them as well! For even more resources, my DMs are open! ","categories": ["Experience"],
        "tags": ["Interview Preparation"],
        "url": "/experience/lockdown-covid/"
      }]
